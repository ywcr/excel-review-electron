"""
å¿«é€Ÿæµ‹è¯• CLIP å­£èŠ‚æ£€æµ‹ (ç›´æ¥è®¡ç®—ç‰ˆ)
ç”¨æ³•: python scripts/test_season.py
"""
import os
import numpy as np
from PIL import Image

# å°è¯•å¯¼å…¥å¿…è¦åº“
try:
    import onnxruntime as ort
    import torch
    from transformers import CLIPModel, CLIPTokenizer
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–: {e}")
    print("è¯·å®‰è£…: pip install onnxruntime torch transformers")
    exit(1)

# è·¯å¾„
SCRIPT_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(SCRIPT_DIR, "..", "electron", "models")
VISUAL_MODEL = os.path.join(MODEL_DIR, "clip-visual-fp16.onnx")

# ç©¿ç€å­£èŠ‚ prompts (ç®€åŒ–ç‰ˆ)
CLOTHING_PROMPTS = [
    "person in puffy down jacket or thick winter coat",
    "person in sweater, cardigan, or light coat",
    "person in t-shirt, shorts, or summer dress",
    "person in thin jacket or light clothes",
    "no person in the image",
]

SEASONS = ["winter", "autumn", "summer", "spring", "no_person"]

# é¢„å¤„ç†å›¾ç‰‡
def preprocess_image(image_path):
    img = Image.open(image_path).convert("RGB")
    img = img.resize((224, 224), Image.Resampling.BILINEAR)
    img_array = np.array(img, dtype=np.float32) / 255.0
    mean = np.array([0.48145466, 0.4578275, 0.40821073])
    std = np.array([0.26862954, 0.26130258, 0.27577711])
    img_array = (img_array - mean) / std
    img_array = img_array.transpose(2, 0, 1)
    img_array = np.expand_dims(img_array, axis=0).astype(np.float32)
    return img_array

def test_image(visual_session, text_embeddings, image_path):
    print(f"\nğŸ“· æµ‹è¯•: {os.path.basename(image_path)}")
    
    input_tensor = preprocess_image(image_path)
    outputs = visual_session.run(None, {"input": input_tensor})
    image_embedding = outputs[0][0]
    image_embedding = image_embedding / np.linalg.norm(image_embedding)
    
    print("   ç©¿ç€å­£èŠ‚åˆ†æ•°:")
    scores = []
    for i, prompt in enumerate(CLOTHING_PROMPTS):
        text_emb = text_embeddings[i]
        similarity = np.dot(image_embedding, text_emb)
        scores.append(similarity)
        print(f"   {SEASONS[i]:10s}: {similarity*100:.1f}%")
    
    max_idx = np.argmax(scores[:4])
    no_person_score = scores[4]
    max_clothing_score = scores[max_idx]
    
    if no_person_score > max_clothing_score:
        print(f"   -> ç»“æœ: æ— äºº (no_person: {no_person_score*100:.1f}% > æœ€é«˜æœé¥°: {max_clothing_score*100:.1f}%)")
    else:
        print(f"   -> ç»“æœ: {SEASONS[max_idx]} ({max_clothing_score*100:.1f}%)")

def main():
    print("=== CLIP å­£èŠ‚æ£€æµ‹å¿«é€Ÿæµ‹è¯• (ç›´æ¥è®¡ç®—ç‰ˆ) ===")
    
    # åŠ è½½æ–‡æœ¬ç¼–ç å™¨
    print("åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨...")
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32", use_safetensors=True)
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
    clip_model.eval()
    
    # è®¡ç®—æ–‡æœ¬ embeddings
    print("è®¡ç®—ç©¿ç€å­£èŠ‚æ–‡æœ¬ embeddings...")
    text_embeddings = []
    for prompt in CLOTHING_PROMPTS:
        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            text_embeds = clip_model.get_text_features(**inputs)
            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
            text_embeddings.append(text_embeds[0].numpy())
    
    # åŠ è½½è§†è§‰æ¨¡å‹
    print(f"åŠ è½½ CLIP è§†è§‰æ¨¡å‹...")
    visual_session = ort.InferenceSession(VISUAL_MODEL, providers=["CPUExecutionProvider"])
    print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
    
    # æµ‹è¯•å›¾ç‰‡
    test_images = [
        "debug_images/image2.jpeg",
        "debug_images/image3.jpeg",
        "debug_images/image6.jpeg",
        "debug_images/image9.jpeg",
    ]
    
    base_dir = os.path.join(SCRIPT_DIR, "..")
    
    for img in test_images:
        full_path = os.path.join(base_dir, img)
        if os.path.exists(full_path):
            test_image(visual_session, text_embeddings, full_path)
        else:
            print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {img}")

if __name__ == "__main__":
    main()
