"""
æ›´æ–° text-embeddings.jsonï¼Œæ·»åŠ æ–°çš„æ°´å°æ£€æµ‹ prompts
ä½¿ç”¨ safetensors åŠ è½½æ¨¡å‹ä»¥å…¼å®¹è¾ƒè€çš„ torch ç‰ˆæœ¬
"""
import os
import json
import torch
from transformers import CLIPModel, CLIPTokenizer

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "electron/models"
EMBEDDINGS_PATH = os.path.join(OUTPUT_DIR, "text-embeddings.json")

print("ğŸ”„ åŠ è½½ CLIP æ¨¡å‹ (ä½¿ç”¨ safetensors)...")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32", use_safetensors=True)
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
model.eval()

# æ–°å¢çš„ prompts
new_texts = [
    "a photo with semi-transparent text or logo watermark",
    "a photo with faint watermark in the corner",
    # æ–°å¢çš„å­£èŠ‚æ£€æµ‹æç¤ºè¯ï¼ˆæ˜¥å¤ç§‹å†¬å››å­£ï¼‰
    "winter scenery with bare leafless trees, snow, frost, dry branches or cold weather",
    "autumn scenery with golden yellow and orange leaves falling from trees",
    "summer scenery with dense green foliage, bright sunshine and blue sky",
    "spring scenery with cherry blossoms, colorful flowers blooming, and fresh green leaves",
]

# éœ€è¦åˆ é™¤çš„æ—§æç¤ºè¯
old_texts_to_remove = [
    "winter scenery with snow, bare trees, frost, or ice",
    "autumn scenery with yellow, orange, or red falling leaves",
    "summer scenery with lush green trees and bright sunshine",
    "spring scenery with blooming flowers and fresh green buds",
]

# åŠ è½½ç°æœ‰çš„åµŒå…¥
existing_embeddings = {}
if os.path.exists(EMBEDDINGS_PATH):
    with open(EMBEDDINGS_PATH, "r", encoding="utf-8") as f:
        existing_embeddings = json.load(f)
    print(f"ğŸ“š å·²åŠ è½½ {len(existing_embeddings)} ä¸ªç°æœ‰åµŒå…¥")

# åˆ é™¤æ—§çš„æç¤ºè¯åµŒå…¥
for old_text in old_texts_to_remove:
    if old_text in existing_embeddings:
        del existing_embeddings[old_text]
        print(f"  ğŸ—‘ï¸ å·²åˆ é™¤æ—§åµŒå…¥: {old_text[:40]}...")

# è®¡ç®—æ–°çš„åµŒå…¥
print("ğŸ“ è®¡ç®—æ–°çš„æ–‡æœ¬åµŒå…¥...")
for text in new_texts:
    if text in existing_embeddings:
        print(f"  â­ï¸ è·³è¿‡ (å·²å­˜åœ¨): {text[:40]}...")
        continue
    
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        text_embeds = model.get_text_features(**inputs)
        # å½’ä¸€åŒ–
        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
        existing_embeddings[text] = text_embeds[0].tolist()
    print(f"  âœ“ {text[:50]}...")

# ä¿å­˜æ›´æ–°åçš„åµŒå…¥
with open(EMBEDDINGS_PATH, "w", encoding="utf-8") as f:
    json.dump(existing_embeddings, f)
print(f"âœ… æ–‡æœ¬åµŒå…¥å·²ä¿å­˜: {EMBEDDINGS_PATH} ({len(existing_embeddings)} ä¸ª)")

print("\nğŸ‰ å®Œæˆ!")
