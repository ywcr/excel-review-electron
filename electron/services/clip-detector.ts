/**
 * CLIP å›¾ç‰‡æ™ºèƒ½æ£€æµ‹æœåŠ¡
 * ä½¿ç”¨ OpenAI CLIP æ¨¡å‹è¿›è¡Œæ°´å°æ£€æµ‹å’Œå­£èŠ‚è¯†åˆ«
 */
// Use dynamic require to bypass Rollup bundling for native module
import type * as OnnxRuntime from "onnxruntime-node";
// eslint-disable-next-line @typescript-eslint/no-var-requires
const ort: typeof OnnxRuntime = require("onnxruntime-node");
import * as path from "path";
import * as fs from "fs";
import sharp from "sharp";

// å­£èŠ‚å®šä¹‰ï¼ˆä¸­å›½åŒ—æ–¹ï¼‰
export type Season = "spring" | "summer" | "autumn" | "winter" | "unknown";

export interface ClipDetectionResult {
  // æ°´å°æ£€æµ‹
  hasWatermark: boolean;
  watermarkConfidence: number;

  // å­£èŠ‚æ£€æµ‹
  detectedSeason: Season;
  seasonConfidence: number;
  clothingSeason?: Season;
  scenerySeason?: Season;

  // æ¨¡ç³Šæ£€æµ‹
  isBlurry: boolean;
  blurConfidence: number;

  // åŸå§‹åˆ†æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
  rawScores?: Record<string, number>;
}

// æ£€æµ‹ prompts
const WATERMARK_PROMPTS = [
  "a photo with visible watermark or logo overlay",
  "a clean photo without any watermark",
];

const CLOTHING_PROMPTS = [
  "person wearing heavy winter clothes like down jacket, coat, or scarf",
  "person wearing autumn clothes like sweater or light jacket",
  "person wearing summer clothes like t-shirt, shorts, or dress",
  "person wearing spring clothes like thin jacket or long sleeves",
  "no person visible in the image",
];

const SCENERY_PROMPTS = [
  "winter scenery with snow, bare trees, frost, or ice",
  "autumn scenery with yellow, orange, or red falling leaves",
  "summer scenery with lush green trees and bright sunshine",
  "spring scenery with blooming flowers and fresh green buds",
  "indoor scene or no natural scenery visible",
];

// æ¨¡ç³Šæ£€æµ‹ prompts
const BLUR_PROMPTS = [
  "a blurry, out of focus, or motion blurred photo",
  "a sharp, clear, and in-focus photo",
];

export class ClipDetector {
  private visualSession: OnnxRuntime.InferenceSession | null = null;
  private textSession: OnnxRuntime.InferenceSession | null = null;
  private textEmbeddings: Map<string, Float32Array> = new Map();
  private isInitialized = false;
  private modelDir: string;

  constructor() {
    // æ¨¡å‹ç›®å½•ï¼šå¼€å‘ç¯å¢ƒåœ¨ electron/modelsï¼Œæ‰“åŒ…ååœ¨ resources/models
    // æ³¨æ„ï¼šå¼€å‘æ¨¡å¼ä¸‹ process.resourcesPath ä¹Ÿå­˜åœ¨ï¼Œä½†æŒ‡å‘ Electron åŒ…ç›®å½•
    // å› æ­¤éœ€è¦ä½¿ç”¨ app.isPackaged æˆ–æ£€æŸ¥ ELECTRON_DEV ç¯å¢ƒå˜é‡
    const isDev = process.env.NODE_ENV === "development" || 
                  !process.resourcesPath?.includes("app.asar") ||
                  process.resourcesPath?.includes("node_modules");
    
    if (isDev) {
      // å¼€å‘æ¨¡å¼ï¼šä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ electron/models
      // __dirname åœ¨ç¼–è¯‘åå¯èƒ½æ˜¯ dist-electronï¼Œéœ€è¦å›åˆ°é¡¹ç›®æ ¹ç›®å½•
      this.modelDir = path.join(process.cwd(), "electron", "models");
    } else {
      // æ‰“åŒ…åï¼šä½¿ç”¨ resources/models
      this.modelDir = path.join(process.resourcesPath!, "models");
    }
    console.log(`ğŸ“‚ [CLIP] æ¨¡å‹ç›®å½•: ${this.modelDir} (å¼€å‘æ¨¡å¼: ${isDev})`);
  }

  /**
   * åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰
   */
  async initialize(): Promise<boolean> {
    if (this.isInitialized) return true;

    const visualModelPath = path.join(this.modelDir, "clip-visual.onnx");
    const textModelPath = path.join(this.modelDir, "clip-textual.onnx");
    const embeddingsPath = path.join(this.modelDir, "text-embeddings.json");

    // æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if (!fs.existsSync(visualModelPath)) {
      console.warn(
        `âš ï¸ CLIP visual model not found at ${visualModelPath}. Detection disabled.`
      );
      return false;
    }

    try {
      console.log("ğŸ”„ Loading CLIP visual model...");
      this.visualSession = await ort.InferenceSession.create(visualModelPath, {
        executionProviders: ["cpu"],
      });

      // åŠ è½½é¢„è®¡ç®—çš„æ–‡æœ¬åµŒå…¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
      if (fs.existsSync(embeddingsPath)) {
        console.log("ğŸ”„ Loading pre-computed text embeddings...");
        const embeddings = JSON.parse(fs.readFileSync(embeddingsPath, "utf-8"));
        for (const [text, values] of Object.entries(embeddings)) {
          this.textEmbeddings.set(text, new Float32Array(values as number[]));
        }
        console.log(`ğŸ“š Loaded ${this.textEmbeddings.size} text embeddings`);
      } else if (fs.existsSync(textModelPath)) {
        // å¦‚æœæœ‰æ–‡æœ¬æ¨¡å‹ï¼ŒåŠ¨æ€è®¡ç®—åµŒå…¥
        console.log("ğŸ”„ Loading CLIP text model...");
        this.textSession = await ort.InferenceSession.create(textModelPath, {
          executionProviders: ["cpu"],
        });
        // é¢„è®¡ç®—æ‰€æœ‰éœ€è¦çš„æ–‡æœ¬åµŒå…¥
        await this.precomputeTextEmbeddings();
      } else {
        console.warn("âš ï¸ No text embeddings or text model found.");
        return false;
      }

      this.isInitialized = true;
      console.log("âœ… CLIP detector initialized successfully");
      return true;
    } catch (error) {
      console.error("âŒ Failed to initialize CLIP detector:", error);
      return false;
    }
  }

  /**
   * é¢„è®¡ç®—æ‰€æœ‰æ–‡æœ¬åµŒå…¥
   */
  private async precomputeTextEmbeddings(): Promise<void> {
    if (!this.textSession) return;

    const allPrompts = [
      ...WATERMARK_PROMPTS,
      ...CLOTHING_PROMPTS,
      ...SCENERY_PROMPTS,
    ];

    for (const prompt of allPrompts) {
      // ç®€åŒ–çš„æ–‡æœ¬ç¼–ç ï¼ˆå®é™…éœ€è¦ CLIP çš„ tokenizerï¼‰
      // è¿™é‡Œå‡è®¾å·²ç»æœ‰é¢„è®¡ç®—çš„åµŒå…¥
      console.log(`  Computing embedding for: ${prompt.substring(0, 30)}...`);
    }
  }

  /**
   * é¢„å¤„ç†å›¾ç‰‡ä¸º CLIP è¾“å…¥æ ¼å¼
   * CLIP éœ€è¦ 224x224 çš„ RGB å›¾ç‰‡ï¼Œå½’ä¸€åŒ–åˆ° [-1, 1]
   */
  private async preprocessImage(imageBuffer: Buffer): Promise<Float32Array> {
    const { data } = await sharp(imageBuffer)
      .resize(224, 224, { fit: "cover" })
      .removeAlpha()
      .raw()
      .toBuffer({ resolveWithObject: true });

    // CLIP çš„å½’ä¸€åŒ–å‚æ•°
    const mean = [0.48145466, 0.4578275, 0.40821073];
    const std = [0.26862954, 0.26130258, 0.27577711];

    // è½¬æ¢ä¸º CHW æ ¼å¼å¹¶å½’ä¸€åŒ–
    const pixels = new Float32Array(3 * 224 * 224);
    for (let c = 0; c < 3; c++) {
      for (let h = 0; h < 224; h++) {
        for (let w = 0; w < 224; w++) {
          const srcIdx = (h * 224 + w) * 3 + c;
          const dstIdx = c * 224 * 224 + h * 224 + w;
          pixels[dstIdx] = (data[srcIdx] / 255 - mean[c]) / std[c];
        }
      }
    }

    return pixels;
  }

  /**
   * è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
   */
  private cosineSimilarity(a: Float32Array, b: Float32Array): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  /**
   * æ£€æµ‹å›¾ç‰‡
   */
  async detect(imageBuffer: Buffer): Promise<ClipDetectionResult | null> {
    if (!this.isInitialized) {
      const success = await this.initialize();
      if (!success) return null;
    }

    if (!this.visualSession) return null;

    try {
      // 1. é¢„å¤„ç†å›¾ç‰‡
      const pixels = await this.preprocessImage(imageBuffer);
      const inputTensor = new ort.Tensor("float32", pixels, [1, 3, 224, 224]);

      // 2. è·å–å›¾ç‰‡åµŒå…¥
      const outputs = await this.visualSession.run({ input: inputTensor });
      const imageEmbedding = outputs.output.data as Float32Array;

      // 3. è®¡ç®—ä¸å„ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
      const scores: Record<string, number> = {};

      for (const [text, textEmbedding] of this.textEmbeddings) {
        scores[text] = this.cosineSimilarity(imageEmbedding, textEmbedding);
      }

      // 4. è§£æç»“æœ
      const result = this.parseScores(scores);
      
      // è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæ£€æµ‹ç»“æœ
      console.log(`ğŸ” [CLIP] æ°´å°æ£€æµ‹: ${result.hasWatermark ? 'æœ‰æ°´å°' : 'æ— æ°´å°'} (ç½®ä¿¡åº¦: ${result.watermarkConfidence.toFixed(1)}%)`);
      console.log(`ğŸ” [CLIP] æ¨¡ç³Šæ£€æµ‹: ${result.isBlurry ? 'æ¨¡ç³Š' : 'æ¸…æ™°'} (ç½®ä¿¡åº¦: ${result.blurConfidence.toFixed(1)}%)`);
      console.log(`ğŸ” [CLIP] å­£èŠ‚æ£€æµ‹: ${result.detectedSeason} (ç½®ä¿¡åº¦: ${result.seasonConfidence.toFixed(1)}%)`);
      if (result.clothingSeason) console.log(`   - ç©¿ç€å­£èŠ‚: ${result.clothingSeason}`);
      if (result.scenerySeason) console.log(`   - åœºæ™¯å­£èŠ‚: ${result.scenerySeason}`);
      
      return result;
    } catch (error) {
      console.error("CLIP detection failed:", error);
      return null;
    }
  }

  /**
   * è§£æåˆ†æ•°ï¼Œè¿”å›æ£€æµ‹ç»“æœ
   */
  private parseScores(scores: Record<string, number>): ClipDetectionResult {
    // æ°´å°æ£€æµ‹ï¼šæ¯”è¾ƒæœ‰æ°´å° vs æ— æ°´å°çš„åˆ†æ•°
    const watermarkScore = scores[WATERMARK_PROMPTS[0]] || 0;
    const cleanScore = scores[WATERMARK_PROMPTS[1]] || 0;
    const watermarkConfidence = Math.abs(watermarkScore - cleanScore) * 100;
    
    // æ°´å°æ£€æµ‹éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼š
    // 1. "æœ‰æ°´å°"åˆ†æ•° > "æ— æ°´å°"åˆ†æ•°
    // 2. ç½®ä¿¡åº¦ï¼ˆåˆ†æ•°å·®ï¼‰éœ€è¦ >= 10%ï¼Œè¡¨ç¤ºæ¨¡å‹è¶³å¤Ÿç¡®å®š
    const WATERMARK_CONFIDENCE_THRESHOLD = 10;
    const hasWatermark = watermarkScore > cleanScore && watermarkConfidence >= WATERMARK_CONFIDENCE_THRESHOLD;

    // ç©¿ç€å­£èŠ‚æ£€æµ‹
    const clothingScores = [
      { season: "winter" as Season, score: scores[CLOTHING_PROMPTS[0]] || 0 },
      { season: "autumn" as Season, score: scores[CLOTHING_PROMPTS[1]] || 0 },
      { season: "summer" as Season, score: scores[CLOTHING_PROMPTS[2]] || 0 },
      { season: "spring" as Season, score: scores[CLOTHING_PROMPTS[3]] || 0 },
    ];
    const noPersonScore = scores[CLOTHING_PROMPTS[4]] || 0;
    const maxClothingScore = Math.max(...clothingScores.map((c) => c.score));
    const clothingSeason =
      noPersonScore > maxClothingScore
        ? undefined
        : clothingScores.find((c) => c.score === maxClothingScore)?.season;

    // åœºæ™¯å­£èŠ‚æ£€æµ‹
    const sceneryScores = [
      { season: "winter" as Season, score: scores[SCENERY_PROMPTS[0]] || 0 },
      { season: "autumn" as Season, score: scores[SCENERY_PROMPTS[1]] || 0 },
      { season: "summer" as Season, score: scores[SCENERY_PROMPTS[2]] || 0 },
      { season: "spring" as Season, score: scores[SCENERY_PROMPTS[3]] || 0 },
    ];
    const indoorScore = scores[SCENERY_PROMPTS[4]] || 0;
    const maxSceneryScore = Math.max(...sceneryScores.map((s) => s.score));
    const scenerySeason =
      indoorScore > maxSceneryScore
        ? undefined
        : sceneryScores.find((s) => s.score === maxSceneryScore)?.season;

    // ç»¼åˆå­£èŠ‚åˆ¤æ–­ï¼šä¼˜å…ˆåœºæ™¯ï¼Œç„¶åç©¿ç€
    let detectedSeason: Season = "unknown";
    let seasonConfidence = 0;

    if (scenerySeason) {
      detectedSeason = scenerySeason;
      seasonConfidence = maxSceneryScore * 100;
    } else if (clothingSeason) {
      detectedSeason = clothingSeason;
      seasonConfidence = maxClothingScore * 100;
    }

    // æ¨¡ç³Šæ£€æµ‹ï¼šæ¯”è¾ƒæ¨¡ç³Š vs æ¸…æ™°çš„åˆ†æ•°
    const blurryScore = scores[BLUR_PROMPTS[0]] || 0;
    const sharpScore = scores[BLUR_PROMPTS[1]] || 0;
    const blurConfidence = Math.abs(blurryScore - sharpScore) * 100;
    // æ¨¡ç³Šæ£€æµ‹é˜ˆå€¼ï¼šç½®ä¿¡åº¦éœ€è¦ >= 8% æ‰åˆ¤å®š
    const BLUR_CONFIDENCE_THRESHOLD = 8;
    const isBlurry = blurryScore > sharpScore && blurConfidence >= BLUR_CONFIDENCE_THRESHOLD;

    return {
      hasWatermark,
      watermarkConfidence: Math.min(100, watermarkConfidence),
      detectedSeason,
      seasonConfidence: Math.min(100, seasonConfidence),
      clothingSeason,
      scenerySeason,
      isBlurry,
      blurConfidence: Math.min(100, blurConfidence),
      rawScores: scores,
    };
  }

  /**
   * è·å–å›¾ç‰‡çš„åˆ†åŒºåŸŸåµŒå…¥å‘é‡
   * å°†å›¾ç‰‡åˆ†ä¸º gridSize x gridSize ä¸ªåŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸå•ç‹¬è®¡ç®— CLIP åµŒå…¥
   * @param imageBuffer å›¾ç‰‡ Buffer
   * @param gridSize ç½‘æ ¼å¤§å°ï¼Œé»˜è®¤ 3ï¼ˆ3x3 = 9ä¸ªåŒºåŸŸï¼‰
   * @returns åŒºåŸŸåµŒå…¥æ•°ç»„ï¼ŒæŒ‰è¡Œä¼˜å…ˆé¡ºåºï¼ˆå·¦ä¸Šâ†’å³ä¸‹ï¼‰
   */
  async getRegionalEmbeddings(
    imageBuffer: Buffer,
    gridSize: number = 3
  ): Promise<Float32Array[] | null> {
    if (!this.isInitialized) {
      const success = await this.initialize();
      if (!success) return null;
    }

    if (!this.visualSession) return null;

    try {
      // 1. è·å–åŸå›¾å°ºå¯¸
      const metadata = await sharp(imageBuffer).metadata();
      if (!metadata.width || !metadata.height) return null;

      const regionWidth = Math.floor(metadata.width / gridSize);
      const regionHeight = Math.floor(metadata.height / gridSize);
      const embeddings: Float32Array[] = [];

      // 2. æå–æ¯ä¸ªåŒºåŸŸå¹¶è®¡ç®—åµŒå…¥
      for (let row = 0; row < gridSize; row++) {
        for (let col = 0; col < gridSize; col++) {
          const left = col * regionWidth;
          const top = row * regionHeight;

          // è£å‰ªåŒºåŸŸ
          const regionBuffer = await sharp(imageBuffer)
            .extract({
              left,
              top,
              width: regionWidth,
              height: regionHeight,
            })
            .toBuffer();

          // è·å–åŒºåŸŸåµŒå…¥
          const embedding = await this.getImageEmbedding(regionBuffer);
          if (embedding) {
            embeddings.push(embedding);
          } else {
            // å¦‚æœè·å–å¤±è´¥ï¼Œè¿”å›ç©ºå‘é‡
            embeddings.push(new Float32Array(512));
          }
        }
      }

      console.log(`ğŸ”³ [CLIP] åŒºåŸŸåµŒå…¥: ç”Ÿæˆ ${gridSize}x${gridSize}=${embeddings.length} ä¸ªåŒºåŸŸåµŒå…¥`);
      return embeddings;
    } catch (error) {
      console.error("è·å–åŒºåŸŸåµŒå…¥å¤±è´¥:", error);
      return null;
    }
  }

  /**
   * è·å–å•å¼ å›¾ç‰‡çš„ CLIP åµŒå…¥å‘é‡
   * @param imageBuffer å›¾ç‰‡ Buffer
   * @returns 512 ç»´åµŒå…¥å‘é‡
   */
  async getImageEmbedding(imageBuffer: Buffer): Promise<Float32Array | null> {
    if (!this.isInitialized) {
      const success = await this.initialize();
      if (!success) return null;
    }

    if (!this.visualSession) return null;

    try {
      const pixels = await this.preprocessImage(imageBuffer);
      const inputTensor = new ort.Tensor("float32", pixels, [1, 3, 224, 224]);
      const outputs = await this.visualSession.run({ input: inputTensor });
      return outputs.output.data as Float32Array;
    } catch (error) {
      console.error("è·å–å›¾ç‰‡åµŒå…¥å¤±è´¥:", error);
      return null;
    }
  }

  /**
   * è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡çš„ç›¸ä¼¼åº¦ï¼ˆå…¬å¼€æ–¹æ³•ï¼‰
   */
  calculateSimilarity(a: Float32Array, b: Float32Array): number {
    return this.cosineSimilarity(a, b);
  }

  /**
   * è·å–åˆå§‹åŒ–çŠ¶æ€
   */
  isReady(): boolean {
    return this.isInitialized;
  }

  /**
   * å…³é—­ä¼šè¯
   */
  async dispose(): Promise<void> {
    if (this.visualSession) {
      await this.visualSession.release();
      this.visualSession = null;
    }
    if (this.textSession) {
      await this.textSession.release();
      this.textSession = null;
    }
    this.isInitialized = false;
  }
}

// å•ä¾‹
let clipDetectorInstance: ClipDetector | null = null;

export function getClipDetector(): ClipDetector {
  if (!clipDetectorInstance) {
    clipDetectorInstance = new ClipDetector();
  }
  return clipDetectorInstance;
}
